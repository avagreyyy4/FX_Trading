{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "69f08f2e-f0af-4790-8620-d8800b40d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING LIBRARIES \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "import sklearn.datasets\n",
    "import sklearn.preprocessing\n",
    "import sklearn.random_projection\n",
    "import sklearn.neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "9ceed0a2-bee6-4074-a6bd-f4e9be16c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting with these parameters\n",
    "n = 21\n",
    "profit_taking = 0.0020\n",
    "p = 0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "9e10a2df-8487-4039-87d2-82e35fc0e9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Change %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2629</th>\n",
       "      <td>2005-09-16</td>\n",
       "      <td>0.6765</td>\n",
       "      <td>0.6768</td>\n",
       "      <td>0.6790</td>\n",
       "      <td>0.6746</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628</th>\n",
       "      <td>2005-09-19</td>\n",
       "      <td>0.6735</td>\n",
       "      <td>0.6747</td>\n",
       "      <td>0.6759</td>\n",
       "      <td>0.6723</td>\n",
       "      <td>-0.44%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2627</th>\n",
       "      <td>2005-09-20</td>\n",
       "      <td>0.6736</td>\n",
       "      <td>0.6734</td>\n",
       "      <td>0.6753</td>\n",
       "      <td>0.6725</td>\n",
       "      <td>0.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2626</th>\n",
       "      <td>2005-09-21</td>\n",
       "      <td>0.6750</td>\n",
       "      <td>0.6737</td>\n",
       "      <td>0.6763</td>\n",
       "      <td>0.6727</td>\n",
       "      <td>0.21%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2625</th>\n",
       "      <td>2005-09-22</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.6750</td>\n",
       "      <td>0.6800</td>\n",
       "      <td>0.6743</td>\n",
       "      <td>0.53%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date   Close    Open    High     Low Change %\n",
       "2629 2005-09-16  0.6765  0.6768  0.6790  0.6746    0.00%\n",
       "2628 2005-09-19  0.6735  0.6747  0.6759  0.6723   -0.44%\n",
       "2627 2005-09-20  0.6736  0.6734  0.6753  0.6725    0.01%\n",
       "2626 2005-09-21  0.6750  0.6737  0.6763  0.6727    0.21%\n",
       "2625 2005-09-22  0.6786  0.6750  0.6800  0.6743    0.53%"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# READ IN DATA\n",
    "df_second_ten = pd.read_csv(\"data/EUR_GBP_2015_2005.csv\")\n",
    "df_first_ten = pd.read_csv(\"data/EUR_GBP_2025_2015.csv\")\n",
    "\n",
    "df_original = pd.concat([df_second_ten, df_first_ten], ignore_index = True)\n",
    "\n",
    "df_original[\"Date\"] = pd.to_datetime(df_original[\"Date\"], format=\"%m/%d/%y\")\n",
    "\n",
    "df_original = df_original.sort_values(by =  \"Date\", ascending=True)\n",
    "\n",
    "df_original = df_original.rename(columns = {'Price': 'Close'})\n",
    "df_original = df_original[['Date', 'Close', 'Open', 'High', 'Low', 'Change %']]\n",
    "df = df_original[['Date', 'Close', 'Open', 'High', 'Low', 'Change %']].copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "cf18f14a-af67-42b8-bbb0-ab97c083c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"] = (df[\"High\"] > (profit_taking+1)*df[\"Open\"]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "6e5c0f84-6ae8-443f-9ce0-b6dc06187f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE PRIOR n DAYS FEATURE\n",
    "\n",
    "for before in range(1, n+1):\n",
    "    df[f\"Close_{before}_before\"] = df[\"Close\"].shift(before)\n",
    "    df[f\"Open_{before}_before\"] = df[\"Open\"].shift(before)\n",
    "    df[f\"High_{before}_before\"] = df[\"High\"].shift(before)\n",
    "    df[f\"Low_{before}_before\"] = df[\"Low\"].shift(before)\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "4aad8069-9747-4c90-b124-7b64f73a0441",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df   = df[(df['Date'] >= '2016-01-01') & (df['Date'] < '2020-01-01')]\n",
    "val_df   = df[(df['Date'] >= '2020-01-01') & (df['Date'] < '2022-01-01')]\n",
    "test_df  = df[df['Date'] >= '2022-01-01']\n",
    "\n",
    "x_train = train_df.drop(columns=['target', 'Date', 'Change %', 'Close', 'Open', 'High', 'Low'])\n",
    "y_train = train_df['target']\n",
    "\n",
    "x_val = val_df.drop(columns=['target', 'Date', 'Change %', 'Close', 'Open', 'High', 'Low'])\n",
    "y_val = val_df['target']\n",
    "\n",
    "x_test = test_df.drop(columns=['target', 'Date', 'Change %', 'Close', 'Open', 'High', 'Low'])\n",
    "y_test = test_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "091e41eb-1a10-40b5-aacc-5e5fce2bbb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1043, 84)\n",
      "(523, 84)\n",
      "(1011, 84)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "2d6fe556-b2f1-4774-b80c-f0f5087c1b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)   \n",
    "\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val   = scaler.transform(x_val)\n",
    "x_test  = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "4a0775a3-d39a-4a1c-ad47-e5d869dfc0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66003072\n",
      "Iteration 2, loss = 0.61549174\n",
      "Iteration 3, loss = 0.60671859\n",
      "Iteration 4, loss = 0.59340397\n",
      "Iteration 5, loss = 0.58732321\n",
      "Iteration 6, loss = 0.58048856\n",
      "Iteration 7, loss = 0.58401356\n",
      "Iteration 8, loss = 0.57647641\n",
      "Iteration 9, loss = 0.57452040\n",
      "Iteration 10, loss = 0.57310816\n",
      "Iteration 11, loss = 0.57264472\n",
      "Iteration 12, loss = 0.57252680\n",
      "Iteration 13, loss = 0.57267629\n",
      "Iteration 14, loss = 0.57191362\n",
      "Iteration 15, loss = 0.57084916\n",
      "Iteration 16, loss = 0.57319400\n",
      "Iteration 17, loss = 0.56365881\n",
      "Iteration 18, loss = 0.56631577\n",
      "Iteration 19, loss = 0.56296689\n",
      "Iteration 20, loss = 0.56333461\n",
      "Iteration 21, loss = 0.56087272\n",
      "Iteration 22, loss = 0.56273591\n",
      "Iteration 23, loss = 0.56558686\n",
      "Iteration 24, loss = 0.56141786\n",
      "Iteration 25, loss = 0.55710120\n",
      "Iteration 26, loss = 0.55438550\n",
      "Iteration 27, loss = 0.55706885\n",
      "Iteration 28, loss = 0.55718008\n",
      "Iteration 29, loss = 0.56449533\n",
      "Iteration 30, loss = 0.56836403\n",
      "Iteration 31, loss = 0.55378288\n",
      "Iteration 32, loss = 0.55573134\n",
      "Iteration 33, loss = 0.55223022\n",
      "Iteration 34, loss = 0.55563832\n",
      "Iteration 35, loss = 0.55598172\n",
      "Iteration 36, loss = 0.56059144\n",
      "Iteration 37, loss = 0.55035506\n",
      "Iteration 38, loss = 0.55637960\n",
      "Iteration 39, loss = 0.54699350\n",
      "Iteration 40, loss = 0.54959257\n",
      "Iteration 41, loss = 0.54814776\n",
      "Iteration 42, loss = 0.54338459\n",
      "Iteration 43, loss = 0.55109770\n",
      "Iteration 44, loss = 0.54064787\n",
      "Iteration 45, loss = 0.54108677\n",
      "Iteration 46, loss = 0.53897140\n",
      "Iteration 47, loss = 0.54156947\n",
      "Iteration 48, loss = 0.53686769\n",
      "Iteration 49, loss = 0.53988674\n",
      "Iteration 50, loss = 0.55367983\n",
      "Iteration 51, loss = 0.55001019\n",
      "Iteration 52, loss = 0.54324989\n",
      "Iteration 53, loss = 0.55737316\n",
      "Iteration 54, loss = 0.53588399\n",
      "Iteration 55, loss = 0.53683338\n",
      "Iteration 56, loss = 0.53685262\n",
      "Iteration 57, loss = 0.53508445\n",
      "Iteration 58, loss = 0.53363402\n",
      "Iteration 59, loss = 0.53512974\n",
      "Iteration 60, loss = 0.53522758\n",
      "Iteration 61, loss = 0.53752379\n",
      "Iteration 62, loss = 0.52910540\n",
      "Iteration 63, loss = 0.53157246\n",
      "Iteration 64, loss = 0.52293915\n",
      "Iteration 65, loss = 0.51914328\n",
      "Iteration 66, loss = 0.51926150\n",
      "Iteration 67, loss = 0.52542344\n",
      "Iteration 68, loss = 0.52230576\n",
      "Iteration 69, loss = 0.52586902\n",
      "Iteration 70, loss = 0.51833364\n",
      "Iteration 71, loss = 0.52548751\n",
      "Iteration 72, loss = 0.51491318\n",
      "Iteration 73, loss = 0.51746374\n",
      "Iteration 74, loss = 0.50964095\n",
      "Iteration 75, loss = 0.51434381\n",
      "Iteration 76, loss = 0.53120692\n",
      "Iteration 77, loss = 0.52415952\n",
      "Iteration 78, loss = 0.51691517\n",
      "Iteration 79, loss = 0.50911580\n",
      "Iteration 80, loss = 0.50288169\n",
      "Iteration 81, loss = 0.50096101\n",
      "Iteration 82, loss = 0.50818568\n",
      "Iteration 83, loss = 0.50655756\n",
      "Iteration 84, loss = 0.51144681\n",
      "Iteration 85, loss = 0.50202705\n",
      "Iteration 86, loss = 0.50969727\n",
      "Iteration 87, loss = 0.50242383\n",
      "Iteration 88, loss = 0.50894503\n",
      "Iteration 89, loss = 0.49408195\n",
      "Iteration 90, loss = 0.49379554\n",
      "Iteration 91, loss = 0.49249332\n",
      "Iteration 92, loss = 0.49072158\n",
      "Iteration 93, loss = 0.49288664\n",
      "Iteration 94, loss = 0.48574209\n",
      "Iteration 95, loss = 0.48032985\n",
      "Iteration 96, loss = 0.48973572\n",
      "Iteration 97, loss = 0.48380413\n",
      "Iteration 98, loss = 0.48977871\n",
      "Iteration 99, loss = 0.49675637\n",
      "Iteration 100, loss = 0.48805699\n",
      "Iteration 101, loss = 0.49463801\n",
      "Iteration 102, loss = 0.50321327\n",
      "Iteration 103, loss = 0.47268379\n",
      "Iteration 104, loss = 0.48748631\n",
      "Iteration 105, loss = 0.47439223\n",
      "Iteration 106, loss = 0.47531321\n",
      "Iteration 107, loss = 0.48189882\n",
      "Iteration 108, loss = 0.46952788\n",
      "Iteration 109, loss = 0.47804728\n",
      "Iteration 110, loss = 0.47527220\n",
      "Iteration 111, loss = 0.46223863\n",
      "Iteration 112, loss = 0.46303771\n",
      "Iteration 113, loss = 0.46108464\n",
      "Iteration 114, loss = 0.45718946\n",
      "Iteration 115, loss = 0.46587159\n",
      "Iteration 116, loss = 0.45416045\n",
      "Iteration 117, loss = 0.45314514\n",
      "Iteration 118, loss = 0.45365613\n",
      "Iteration 119, loss = 0.44637294\n",
      "Iteration 120, loss = 0.44840732\n",
      "Iteration 121, loss = 0.44551160\n",
      "Iteration 122, loss = 0.44264422\n",
      "Iteration 123, loss = 0.45435226\n",
      "Iteration 124, loss = 0.46024157\n",
      "Iteration 125, loss = 0.46414331\n",
      "Iteration 126, loss = 0.46948703\n",
      "Iteration 127, loss = 0.44207701\n",
      "Iteration 128, loss = 0.43788443\n",
      "Iteration 129, loss = 0.43334189\n",
      "Iteration 130, loss = 0.43769307\n",
      "Iteration 131, loss = 0.43680552\n",
      "Iteration 132, loss = 0.44712907\n",
      "Iteration 133, loss = 0.44173590\n",
      "Iteration 134, loss = 0.43201331\n",
      "Iteration 135, loss = 0.43851470\n",
      "Iteration 136, loss = 0.42809754\n",
      "Iteration 137, loss = 0.42312000\n",
      "Iteration 138, loss = 0.42174456\n",
      "Iteration 139, loss = 0.43099776\n",
      "Iteration 140, loss = 0.41872380\n",
      "Iteration 141, loss = 0.41878600\n",
      "Iteration 142, loss = 0.41899829\n",
      "Iteration 143, loss = 0.41748690\n",
      "Iteration 144, loss = 0.41572965\n",
      "Iteration 145, loss = 0.41401194\n",
      "Iteration 146, loss = 0.40984294\n",
      "Iteration 147, loss = 0.40779708\n",
      "Iteration 148, loss = 0.41612094\n",
      "Iteration 149, loss = 0.40877855\n",
      "Iteration 150, loss = 0.40595372\n",
      "Iteration 151, loss = 0.40905820\n",
      "Iteration 152, loss = 0.40308758\n",
      "Iteration 153, loss = 0.40250529\n",
      "Iteration 154, loss = 0.40782380\n",
      "Iteration 155, loss = 0.39642945\n",
      "Iteration 156, loss = 0.40522387\n",
      "Iteration 157, loss = 0.39476839\n",
      "Iteration 158, loss = 0.41534650\n",
      "Iteration 159, loss = 0.40543022\n",
      "Iteration 160, loss = 0.39493446\n",
      "Iteration 161, loss = 0.39506602\n",
      "Iteration 162, loss = 0.40152799\n",
      "Iteration 163, loss = 0.40006347\n",
      "Iteration 164, loss = 0.38583104\n",
      "Iteration 165, loss = 0.38143870\n",
      "Iteration 166, loss = 0.37738816\n",
      "Iteration 167, loss = 0.38339549\n",
      "Iteration 168, loss = 0.38474466\n",
      "Iteration 169, loss = 0.37846263\n",
      "Iteration 170, loss = 0.40210973\n",
      "Iteration 171, loss = 0.39139466\n",
      "Iteration 172, loss = 0.38296476\n",
      "Iteration 173, loss = 0.37833431\n",
      "Iteration 174, loss = 0.36683710\n",
      "Iteration 175, loss = 0.39222838\n",
      "Iteration 176, loss = 0.38788292\n",
      "Iteration 177, loss = 0.38389887\n",
      "Iteration 178, loss = 0.38228001\n",
      "Iteration 179, loss = 0.37160623\n",
      "Iteration 180, loss = 0.36276304\n",
      "Iteration 181, loss = 0.37186815\n",
      "Iteration 182, loss = 0.36782690\n",
      "Iteration 183, loss = 0.37948544\n",
      "Iteration 184, loss = 0.36555811\n",
      "Iteration 185, loss = 0.37283522\n",
      "Iteration 186, loss = 0.36852121\n",
      "Iteration 187, loss = 0.35733747\n",
      "Iteration 188, loss = 0.36709307\n",
      "Iteration 189, loss = 0.34766057\n",
      "Iteration 190, loss = 0.36480504\n",
      "Iteration 191, loss = 0.35010058\n",
      "Iteration 192, loss = 0.34980389\n",
      "Iteration 193, loss = 0.34763861\n",
      "Iteration 194, loss = 0.34155552\n",
      "Iteration 195, loss = 0.34984231\n",
      "Iteration 196, loss = 0.34450372\n",
      "Iteration 197, loss = 0.34286084\n",
      "Iteration 198, loss = 0.34618495\n",
      "Iteration 199, loss = 0.33767516\n",
      "Iteration 200, loss = 0.33179701\n",
      "Iteration 201, loss = 0.33690621\n",
      "Iteration 202, loss = 0.33323065\n",
      "Iteration 203, loss = 0.33562388\n",
      "Iteration 204, loss = 0.34818887\n",
      "Iteration 205, loss = 0.34466401\n",
      "Iteration 206, loss = 0.32517048\n",
      "Iteration 207, loss = 0.35618656\n",
      "Iteration 208, loss = 0.35549229\n",
      "Iteration 209, loss = 0.33564094\n",
      "Iteration 210, loss = 0.32993505\n",
      "Iteration 211, loss = 0.33390601\n",
      "Iteration 212, loss = 0.33193309\n",
      "Iteration 213, loss = 0.33979377\n",
      "Iteration 214, loss = 0.32642691\n",
      "Iteration 215, loss = 0.33852039\n",
      "Iteration 216, loss = 0.33250085\n",
      "Iteration 217, loss = 0.32243260\n",
      "Iteration 218, loss = 0.33338683\n",
      "Iteration 219, loss = 0.31880930\n",
      "Iteration 220, loss = 0.33152995\n",
      "Iteration 221, loss = 0.32131412\n",
      "Iteration 222, loss = 0.31764872\n",
      "Iteration 223, loss = 0.31922062\n",
      "Iteration 224, loss = 0.31628094\n",
      "Iteration 225, loss = 0.30988651\n",
      "Iteration 226, loss = 0.31391612\n",
      "Iteration 227, loss = 0.30571119\n",
      "Iteration 228, loss = 0.30654121\n",
      "Iteration 229, loss = 0.31140538\n",
      "Iteration 230, loss = 0.31375199\n",
      "Iteration 231, loss = 0.33184311\n",
      "Iteration 232, loss = 0.30212346\n",
      "Iteration 233, loss = 0.30249998\n",
      "Iteration 234, loss = 0.33218952\n",
      "Iteration 235, loss = 0.31754606\n",
      "Iteration 236, loss = 0.30767378\n",
      "Iteration 237, loss = 0.29798477\n",
      "Iteration 238, loss = 0.30691729\n",
      "Iteration 239, loss = 0.31766850\n",
      "Iteration 240, loss = 0.30509197\n",
      "Iteration 241, loss = 0.30806540\n",
      "Iteration 242, loss = 0.29863365\n",
      "Iteration 243, loss = 0.29893604\n",
      "Iteration 244, loss = 0.29137513\n",
      "Iteration 245, loss = 0.29112369\n",
      "Iteration 246, loss = 0.29020080\n",
      "Iteration 247, loss = 0.29601645\n",
      "Iteration 248, loss = 0.31297563\n",
      "Iteration 249, loss = 0.30393443\n",
      "Iteration 250, loss = 0.29717764\n",
      "Iteration 251, loss = 0.29760510\n",
      "Iteration 252, loss = 0.29418293\n",
      "Iteration 253, loss = 0.30465872\n",
      "Iteration 254, loss = 0.29292761\n",
      "Iteration 255, loss = 0.28844871\n",
      "Iteration 256, loss = 0.30189725\n",
      "Iteration 257, loss = 0.29305431\n",
      "Iteration 258, loss = 0.30438818\n",
      "Iteration 259, loss = 0.30823216\n",
      "Iteration 260, loss = 0.30145478\n",
      "Iteration 261, loss = 0.31344923\n",
      "Iteration 262, loss = 0.27935578\n",
      "Iteration 263, loss = 0.27505450\n",
      "Iteration 264, loss = 0.27513544\n",
      "Iteration 265, loss = 0.27054499\n",
      "Iteration 266, loss = 0.26764587\n",
      "Iteration 267, loss = 0.27213089\n",
      "Iteration 268, loss = 0.26915618\n",
      "Iteration 269, loss = 0.31293924\n",
      "Iteration 270, loss = 0.30405354\n",
      "Iteration 271, loss = 0.28955312\n",
      "Iteration 272, loss = 0.29072531\n",
      "Iteration 273, loss = 0.29353427\n",
      "Iteration 274, loss = 0.27808559\n",
      "Iteration 275, loss = 0.27810044\n",
      "Iteration 276, loss = 0.26800980\n",
      "Iteration 277, loss = 0.26613536\n",
      "Iteration 278, loss = 0.27291816\n",
      "Iteration 279, loss = 0.26436894\n",
      "Iteration 280, loss = 0.26596143\n",
      "Iteration 281, loss = 0.27150038\n",
      "Iteration 282, loss = 0.25875853\n",
      "Iteration 283, loss = 0.26169207\n",
      "Iteration 284, loss = 0.25121879\n",
      "Iteration 285, loss = 0.26975270\n",
      "Iteration 286, loss = 0.25767048\n",
      "Iteration 287, loss = 0.25540157\n",
      "Iteration 288, loss = 0.25086235\n",
      "Iteration 289, loss = 0.25349319\n",
      "Iteration 290, loss = 0.25411832\n",
      "Iteration 291, loss = 0.26355516\n",
      "Iteration 292, loss = 0.25562043\n",
      "Iteration 293, loss = 0.24908328\n",
      "Iteration 294, loss = 0.25357945\n",
      "Iteration 295, loss = 0.25185175\n",
      "Iteration 296, loss = 0.24362619\n",
      "Iteration 297, loss = 0.24571853\n",
      "Iteration 298, loss = 0.24144763\n",
      "Iteration 299, loss = 0.24991016\n",
      "Iteration 300, loss = 0.25045003\n",
      "Iteration 301, loss = 0.24568349\n",
      "Iteration 302, loss = 0.24703825\n",
      "Iteration 303, loss = 0.24090116\n",
      "Iteration 304, loss = 0.24703474\n",
      "Iteration 305, loss = 0.25721562\n",
      "Iteration 306, loss = 0.26346867\n",
      "Iteration 307, loss = 0.27537289\n",
      "Iteration 308, loss = 0.26762504\n",
      "Iteration 309, loss = 0.31553411\n",
      "Iteration 310, loss = 0.26971125\n",
      "Iteration 311, loss = 0.29803838\n",
      "Iteration 312, loss = 0.26143960\n",
      "Iteration 313, loss = 0.24257324\n",
      "Iteration 314, loss = 0.24074777\n",
      "Iteration 315, loss = 0.23705730\n",
      "Iteration 316, loss = 0.24298077\n",
      "Iteration 317, loss = 0.24395569\n",
      "Iteration 318, loss = 0.25156089\n",
      "Iteration 319, loss = 0.26014494\n",
      "Iteration 320, loss = 0.24432555\n",
      "Iteration 321, loss = 0.24390885\n",
      "Iteration 322, loss = 0.23514489\n",
      "Iteration 323, loss = 0.23353382\n",
      "Iteration 324, loss = 0.22863384\n",
      "Iteration 325, loss = 0.23653923\n",
      "Iteration 326, loss = 0.25937923\n",
      "Iteration 327, loss = 0.24881483\n",
      "Iteration 328, loss = 0.24126051\n",
      "Iteration 329, loss = 0.24650609\n",
      "Iteration 330, loss = 0.23793322\n",
      "Iteration 331, loss = 0.23575816\n",
      "Iteration 332, loss = 0.22363626\n",
      "Iteration 333, loss = 0.22742950\n",
      "Iteration 334, loss = 0.22695407\n",
      "Iteration 335, loss = 0.22840333\n",
      "Iteration 336, loss = 0.21334821\n",
      "Iteration 337, loss = 0.22231875\n",
      "Iteration 338, loss = 0.22773964\n",
      "Iteration 339, loss = 0.22151051\n",
      "Iteration 340, loss = 0.22589650\n",
      "Iteration 341, loss = 0.22752896\n",
      "Iteration 342, loss = 0.23511592\n",
      "Iteration 343, loss = 0.22547919\n",
      "Iteration 344, loss = 0.21982624\n",
      "Iteration 345, loss = 0.21972473\n",
      "Iteration 346, loss = 0.21461570\n",
      "Iteration 347, loss = 0.23132778\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "validation_accuracy1=0.5736\n",
      "train_accuracy1=0.9175\n",
      "validation_accuracy2=0.5507\n",
      "train_accuracy2=0.8322\n"
     ]
    }
   ],
   "source": [
    "# MODEL TRAINING\n",
    "model1 = sklearn.neural_network.MLPClassifier(\n",
    "    hidden_layer_sizes=[128, 128],\n",
    "    activation='relu',\n",
    "    alpha=1e-3,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6,\n",
    "    verbose=1, \n",
    ")\n",
    "\n",
    "model1.fit(x_train, y_train)\n",
    "\n",
    "validation_accuracy = model1.score(x_val, y_val)\n",
    "print(f\"validation_accuracy1={validation_accuracy:0.4f}\")\n",
    "train_accuracy = model1.score(x_train, y_train)\n",
    "print(f\"train_accuracy1={train_accuracy:0.4f}\")\n",
    "\n",
    "#for k in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]:\n",
    "    \n",
    "    #model2 = sklearn.neighbors.KNeighborsClassifier(\n",
    "    #n_neighbors=k,\n",
    "    #weights='uniform',\n",
    "    #algorithm='ball_tree',\n",
    "    #)\n",
    "\n",
    "    #model2.fit(x_train, y_train)\n",
    "\n",
    "    #validation_accuracy = model2.score(x_val, y_val)\n",
    "    #print(k, f\"validation_accuracy2={validation_accuracy:0.4f}\")\n",
    "    #train_accuracy = model2.score(x_train, y_train)\n",
    "    #print(k, f\"train_accuracy2={train_accuracy:0.4f}\")\n",
    "\n",
    "\n",
    "    \n",
    "model2 = sklearn.neighbors.KNeighborsClassifier(\n",
    "    n_neighbors=3,\n",
    "    weights='uniform',\n",
    "    algorithm='ball_tree',\n",
    "    )\n",
    "\n",
    "model2.fit(x_train, y_train)\n",
    "\n",
    "validation_accuracy = model2.score(x_val, y_val)\n",
    "print(f\"validation_accuracy2={validation_accuracy:0.4f}\")\n",
    "train_accuracy = model2.score(x_train, y_train)\n",
    "print(f\"train_accuracy2={train_accuracy:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "1300362b-fab7-48e0-a005-1f5fff293158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy=0.5608308605341247\n",
      "test_accuracy=0.5539070227497527\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    test_accuracy = model2.score(x_test, y_test)\n",
    "    print(f\"test_accuracy={test_accuracy}\")\n",
    "\n",
    "    y_true = y_test.values.ravel()\n",
    "    probs_test = model2.predict_proba(x_test)[:, 1]\n",
    "    preds_test = model2.predict(x_test)\n",
    "\n",
    "    test_accuracy = model1.score(x_test, y_test)\n",
    "    print(f\"test_accuracy={test_accuracy}\")\n",
    "\n",
    "    y_true = y_test.values.ravel()\n",
    "    probs_test = model1.predict_proba(x_test)[:, 1]\n",
    "    preds_test = model1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "a27994e2-0c1a-453e-8d24-8387b40f28fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['level_0', 'index', 'Date', 'Close', 'Open', 'High', 'Low', 'Change %'], dtype='object')\n",
      "Index(['level_0', 'index', 'target'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_original.columns)\n",
    "print(target.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "a4d0a4a5-d118-41be-8454-8b92427733a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "\n",
    "    # 1. Build feature matrix exactly like training\n",
    "    drop_cols = ['target','Date','Change %','Close','Open','High','Low']\n",
    "    X_all = df.drop(columns=drop_cols)\n",
    "\n",
    "    # 2. Apply the same scaler used earlier\n",
    "    X_all_scaled = scaler.transform(X_all)\n",
    "\n",
    "    # 3. Compute probabilities safely\n",
    "    df_probs = pd.DataFrame({\n",
    "        'index': df.index,\n",
    "        'prob': model1.predict_proba(X_all_scaled)[:, 1]\n",
    "    })\n",
    "\n",
    "    # ⚠️ --- Add these 2 lines to fix the error ---\n",
    "    df_original = df_original.drop(columns=['level_0','index'], errors='ignore')\n",
    "    target      = target.drop(columns=['level_0','index'], errors='ignore')\n",
    "    # --------------------------------------------\n",
    "\n",
    "    # 4. Reset original dfs for merging\n",
    "    df_original = df_original.reset_index()\n",
    "    target      = target.reset_index()\n",
    "\n",
    "    # 5. Merge back\n",
    "    df_evaluation = df_original.merge(df_probs, on='index', how='left')\n",
    "    df_evaluation = df_evaluation.merge(target, on='index', how='left')\n",
    "\n",
    "    # 6. Compute enter column\n",
    "    df_evaluation[\"enter\"] = (df_evaluation[\"prob\"] >= p).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "47962053-c56a-45cc-8c82-7cade2f43903",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    df_evaluation[\"pnl_if_hit\"] = profit_taking\n",
    "    df_evaluation[\"pnl_if_miss\"] = (\n",
    "        (df_evaluation[\"Close\"] - df_evaluation[\"Open\"]) / df_evaluation[\"Open\"]\n",
    "    )\n",
    "\n",
    "    df_evaluation[\"pnl\"] = np.where(\n",
    "        df_evaluation[\"enter\"] == 1,                        \n",
    "        np.where(\n",
    "            df_evaluation[\"target\"] == 1,                   \n",
    "            df_evaluation[\"pnl_if_hit\"],                    \n",
    "            df_evaluation[\"pnl_if_miss\"]                     \n",
    "        ),\n",
    "        0                                                      \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "27ea9b59-d0a4-40a0-9a08-79e9dfd5685b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total return: 4.332270010450365\n",
      "Number of trades: 3620\n",
      "Hit rate: 0.6526696329254728\n",
      "Avg return per trade: 0.0011967596713951284\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    total_return = df_evaluation[\"pnl\"].sum()\n",
    "    num_trades = df_evaluation[\"enter\"].sum()\n",
    "    hit_rate = df_evaluation.loc[df_evaluation[\"enter\"] == 1, \"target\"].mean()\n",
    "    avg_return_per_trade = df_evaluation.loc[df_evaluation[\"enter\"] == 1, \"pnl\"].mean()\n",
    "\n",
    "    print(\"Total return:\", total_return)\n",
    "    print(\"Number of trades:\", num_trades)\n",
    "    print(\"Hit rate:\", hit_rate)\n",
    "    print(\"Avg return per trade:\", avg_return_per_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "4d251fe0-ecb8-432c-92f9-7452ae0b54f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5241.000000\n",
       "mean        0.721483\n",
       "std         0.322450\n",
       "min         0.000028\n",
       "25%         0.550579\n",
       "50%         0.867016\n",
       "75%         0.978962\n",
       "max         1.000000\n",
       "Name: prob, dtype: float64"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluation['prob'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f1e20-5ad7-4735-8894-3c581ec8e04e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
